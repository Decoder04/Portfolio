{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJTr-Sv18Jsp"
      },
      "outputs": [],
      "source": [
        "#! pip install -q --upgrade langchain langchain-openai langchain-core langchain_community docx2txt pypdf  langchain_chroma sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZGjmr9Rl6w",
        "outputId": "b4f1bae5-87e8-43b6-b648-0473d8738bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.16\n"
          ]
        }
      ],
      "source": [
        "## Retrieval augmented generation\n",
        "\n",
        "# open the terminal and change to command prompt\n",
        "# run \"conda create -p venv python==3.10 -y\" to create a new environment\n",
        "# run \"conda activate venv/\" to activate the environment\n",
        "# pip install -r requirements.txt\n",
        "# pip install ipykernel\n",
        "# Then select the same venv as the kernel for this notebook\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../chatbot_rag_langchain/.env\", override=True)\n",
        "\n",
        "import langchain\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynuxRM7_-Nhu"
      },
      "outputs": [],
      "source": [
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\") # Set the API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "#os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "#os.environ[\"LANGCHAIN_PROJECT\"] = \"chatbot-rag1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZLC1ePBkA0Dd"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"chatbot-rag1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from langsmith import utils\n",
        "#utils.tracing_is_enabled()\n",
        "\n",
        "#print (\"API key: \"+os.environ[\"OPENAI_API_KEY\"])\n",
        "#print (\"API key: \"+os.environ[\"LANGSMITH_API_KEY\"])\n",
        "#print (\"API key: \"+os.environ[\"LANGSMITH_TRACING\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGT7MDh-mGk"
      },
      "source": [
        "###Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFPaXjak-Dx6",
        "outputId": "f7fadda0-e604-462d-e61b-42e2d40ead86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-765ab0dc-13a0-414c-bf03-007dc8ce0adb-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "#llm_response = llm.invoke(\"Tell me a joke\")\n",
        "llm_response = llm.invoke(\"Hello, world!\")\n",
        "\n",
        "llm_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv-79m-P_kVF"
      },
      "source": [
        "###Parsing Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g1e1HTs1-vmG",
        "outputId": "b910f96b-8542-4a9f-9274-1a4f31ea1dfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIWdUqOe_nIc"
      },
      "source": [
        "###Simple Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dZAu9MKc_S5z",
        "outputId": "990ebf8f-c4b1-4d98-a542-c06e5f509c80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = llm | output_parser\n",
        "chain.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIRFAy-R57iq"
      },
      "source": [
        "###Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G41Ztxd541C",
        "outputId": "7fec3837-7f36-446e-bc50-f4f7537c5414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MobileReview(phone_model='Samsung Galaxy S21', rating=4.0, pros=['Gorgeous display with vibrant colors', 'Exceptional camera performance, especially in low light', 'Solid battery life lasting all day'], cons=['High price point', 'No charger included in the box', 'New button layout can be confusing, often hitting Bixby by mistake'], summary='The Galaxy S21 impresses with its stunning display and excellent camera, making it a great choice despite some minor annoyances and the high cost.')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "review_text = \"\"\"\n",
        "\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "structured_llm = llm.with_structured_output(MobileReview)\n",
        "output = structured_llm.invoke(review_text)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB-1N7n7n9cz",
        "outputId": "ed375a70-2d1f-4ac5-8d07-ba44561c61be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Gorgeous display with vibrant colors',\n",
              " 'Exceptional camera performance, especially in low light',\n",
              " 'Solid battery life lasting all day']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.pros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpRopsnw_3yo"
      },
      "source": [
        "###Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH9TLudu_-Dw",
        "outputId": "0bd110f8-6b45-43ef-b5bb-2f7baa12c396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Tell me a short joke about programming', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "prompt.invoke({\"topic\": \"programming\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RakPrAV0BqlL",
        "outputId": "9e8c870a-0b12-4532-cd7e-52bd8503d3f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"topic\": \"programer\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pizjNr0h8f_E",
        "outputId": "2ab1230f-7d5c-4ffb-c9d7-0c0f79bffe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why do programmers prefer dark mode? \n",
            "\n",
            "Because light attracts bugs!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define the output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Compose the chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# Use the chain\n",
        "result = chain.invoke({\"topic\": \"programming\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Hl3OjvsA02"
      },
      "source": [
        "###LLM Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37riQcz1FoB_",
        "outputId": "3227525d-8f0b-469d-ac4b-3760895ce870"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Sure! Programming is the process of creating instructions for computers to follow. These instructions are written in programming languages like Python, Java, C++, and many others. \\n\\nBut enough about the technical stuff! Here’s a programming joke for you:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs! 🐛💻', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 24, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-6bb22d91-1554-45b9-b45b-393a0bad4511-0', usage_metadata={'input_tokens': 24, 'output_tokens': 66, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
        "human_message = HumanMessage(content=\"Tell me about programming\")\n",
        "llm.invoke([system_message, human_message])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEufVkCLMm68",
        "outputId": "f8f21040-feae-4a1c-a4c3-297c87d2c9ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a professional consultant working in a HR consulting and training company called Precena Strategic Partners. Use UK English, and a professional and helpful tone while responding. Use frameworks provided in the context to explain your point. Also provide examples after you explain your point. Examples can be from the context provided, or from other training data. Do not respond in a rude or unprofessional tone ever, no matter what the human says. Also never provide personal opinions or experiences. Only provide professional advice and information.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about problem solving', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a professional consultant working in a HR consulting and training company called Precena Strategic Partners. Use UK English, and a professional and helpful tone while responding. Use frameworks provided in the context to explain your point. Also provide examples after you explain your point. Examples can be from the context provided, or from other training data. Do not respond in a rude or unprofessional tone ever, no matter what the human says. Also never provide personal opinions or experiences. Only provide professional advice and information.\"),\n",
        "    (\"human\", \"Tell me about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"topic\": \"problem solving\"\n",
        "    }\n",
        ")\n",
        "prompt_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY86l3k_HwTb",
        "outputId": "995734c5-18f8-4beb-872b-ae9702bd6044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Problem solving is an essential skill in both personal and professional settings. It involves a structured process that helps individuals and teams arrive at effective solutions for various challenges they encounter. Here, I will discuss a popular framework for problem-solving known as the \"Define-Analyze-Solve\" approach, along with practical examples.\\n\\n### 1. Define the Problem\\n\\nThe first step in problem solving is clearly defining the problem at hand. This involves gathering information and identifying the specific issue that needs attention. Being precise in this stage ensures that efforts are targeted effectively.\\n\\n**Example:** In a workplace scenario, if employee productivity is declining, it’s vital to determine whether the issue is due to external factors, such as market changes, or internal factors, like lack of training or low morale.\\n\\n### 2. Analyze the Problem\\n\\nOnce the problem is defined, the next step is to analyze it. This entails investigating the root causes and evaluating the impact of the problem. It may also involve collecting data and insights to understand the situation thoroughly.\\n\\n**Example:** Continuing with the productivity issue, HR may conduct surveys to identify employee satisfaction levels, analyze turnover rates, and assess training programs. This analysis may reveal that a new software implementation has increased the workload and led to confusion among employees, directly impacting productivity.\\n\\n### 3. Solve the Problem\\n\\nThe final step is to generate solutions based on the previously gathered information. This can involve brainstorming sessions, where diverse ideas are encouraged, followed by selecting the most feasible options. It’s also crucial to develop an action plan to implement the chosen solution.\\n\\n**Example:** After identifying the software implementation as a factor, HR might decide to schedule training programs to better equip employees with the new system. Furthermore, they could introduce a feedback mechanism to continuously monitor employee issues related to the software.\\n\\n### Conclusion\\n\\nProblem-solving is a vital competency that can greatly enhance organizational effectiveness and employee engagement. By utilizing the Define-Analyze-Solve framework, teams can systematically address challenges, leading to well-informed decisions that benefit the organization as a whole.\\n\\nIncorporating regular problem-solving sessions within your organization can also nurture a proactive culture. It\\'s a shared competency that fosters collaboration and innovation, enabling teams to adapt more effectively to change. At Precena Strategic Partners, we often emphasize the importance of such frameworks in our training programs, demonstrating how structured problem-solving can lead to better outcomes in various business contexts.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 477, 'prompt_tokens': 76, 'total_tokens': 553, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5a4205e7-e6a0-4af6-9ea0-b1af422cb69d-0', usage_metadata={'input_tokens': 76, 'output_tokens': 477, 'total_tokens': 553, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k3uT6eoTOl7"
      },
      "outputs": [],
      "source": [
        "# !pip install docx2txt pypdf unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-G_ZBmiSJRP",
        "outputId": "d8563a20-61c0-46e4-a84f-43751ef9c115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "Split the documents into 46 chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "#text_splitter = RecursiveCharacterTextSplitter(\n",
        "#    chunk_size=1000,\n",
        "#    chunk_overlap=200,\n",
        "#    length_function=len\n",
        "#)\n",
        "\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
        "\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf\")\n",
        "documents = pdf_loader.load()\n",
        "\n",
        "print(len(documents))\n",
        "\n",
        "#splits = text_splitter.split_documents(documents)\n",
        "#splits = text_splitter.create_documents(documents)\n",
        "\n",
        "\n",
        "#print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaYyDYxKypL7",
        "outputId": "e5a3fbde-0e65-406c-9d0f-231cad1d8732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf', 'page': 0, 'page_label': '1'}, page_content='1\\nFramework for Logical Thinking\\nSummarising\\n(What’s your point?)\\nReasoning\\n(Why?)\\nCovering\\n(Is that all?)\\n')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC5yK58oTa9-",
        "outputId": "27b792fe-6555-480f-df5d-54fd17250a89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'23\\nCovering'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#splits[4]\n",
        "splits[31].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVILJLkITgfz",
        "outputId": "9dce62d3-5976-4919-b149-80ee8beb5f9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf',\n",
              " 'page': 3,\n",
              " 'page_label': '4'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[5].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "LAPIDXucTjFN",
        "outputId": "49523e2f-c352-44e0-d293-f725dac47a59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GreenGrow Innovations was founded in 2010 by Sarah Chen and Michael Rodriguez, two agricultural engineers with a passion for sustainable farming. The company started in a small garage in Portland, Oregon, with a simple mission: to make farming more environmentally friendly and efficient.\\n\\n\\n\\nIn its early days, GreenGrow focused on developing smart irrigation systems that could significantly reduce water usage in agriculture. Their first product, the WaterWise Sensor, was launched in 2012 and quickly gained popularity among local farmers. This success allowed the company to expand its research and development efforts.\\n\\n\\n\\nBy 2015, GreenGrow had outgrown its garage origins and moved into a proper office and research facility in the outskirts of Portland. This move coincided with the development of their second major product, the SoilHealth Monitor, which used advanced sensors to analyze soil composition and provide real-time recommendations for optimal crop growth.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_YJcoYjs8EU"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHPIWFXyTsGM",
        "outputId": "f0ced858-670e-449d-c62a-a7103838efb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 81 documents from the folder.\n",
            "Split the documents into 123 chunks.\n"
          ]
        }
      ],
      "source": [
        "# 1. Function to load documents from a folder\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"../chatbot_rag_langchain/docs/\"\n",
        "documents = load_documents(folder_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf', 'page': 0, 'page_label': '1'}, page_content='1\\nFramework for Logical Thinking\\nSummarising\\n(What’s your point?)\\nReasoning\\n(Why?)\\nCovering\\n(Is that all?)\\n')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "splits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRD0I2D4NSkU",
        "outputId": "0d243630-4507-4d17-9300-8244b932f225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created embeddings for 123 document chunks.\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Embedding Documents\n",
        "\n",
        "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "\n",
        "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKzaMunIUkKZ"
      },
      "outputs": [],
      "source": [
        "document_embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t0XRI1zuTcW"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence_transformers\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
        "#document_embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru-QGlOTWGvv"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain_chroma -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-m02tzkyF35"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# shutil.rmtree('chroma_db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgBuC-Xdu5gL"
      },
      "source": [
        "###Create and persist Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ov-ElhBUpJB",
        "outputId": "8fcd703f-120c-46db-ba55-2bc1423c4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created and persisted to './chroma_db'\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
        "#db.persist()\n",
        "\n",
        "print(\"Vector store created and persisted to './chroma_db'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRqFWA8u3I8",
        "outputId": "31309e8a-a5b0-4d65-8b36-993e8fc440af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 2 most relevant chunks for the query: 'What is covering?'\n",
            "\n",
            "Result 1:\n",
            "Source: ../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf\n",
            "Content: 23\n",
            "Covering\n",
            "\n",
            "Result 2:\n",
            "Source: ../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf\n",
            "Content: 29\n",
            "Class Discussion\n",
            "• How can you apply covering at work? [Reflection] \n",
            "Covering\n",
            "\n",
            "Result 3:\n",
            "Source: ../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf\n",
            "Content: …\n",
            "Output Image – Covering\n",
            "\n",
            "Result 4:\n",
            "Source: ../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf\n",
            "Content: Is that all? Summarising Reasoning Covering\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Testing MMR search\n",
        "\n",
        "query = \"What is the core framework for logical thinking?\"\n",
        "query = \"What is covering?\"\n",
        "#search_results = vectorstore.similarity_search(query, k=3)\n",
        "search_results = vectorstore.max_marginal_relevance_search(query,k=4, fetch_k=6)\n",
        "\n",
        "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
        "for i, result in enumerate(search_results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGkvzzIZvC2R",
        "outputId": "081f1019-f8a3-4dfa-9ed4-22f2d7198cfa"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"fetch_k\": 6})\n",
        "#retriever.invoke(\"What is the core framework for logical thinking?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XRiNA4y8vkcz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#template = \"\"\"Answer the question based only on the following context:\n",
        "#{context}\n",
        "\n",
        "#Question: {question}\n",
        "\n",
        "#Answer: \"\"\"\n",
        "#prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define the prompt template along with system and human messages\n",
        "prompt = ChatPromptTemplate([\n",
        "    (\"system\", \"\"\"You are a professional consultant working in a HR consulting and training company called Precena Strategic Partners.\n",
        "     Refer only to the context provided to answer the question.\n",
        "     Use UK English, and a professional and helpful tone while responding.\n",
        "     Use the most relevant framework provided in the context to explain your point.\n",
        "     Also provide examples after you explain your point. Examples should preferably be from the context provided. If no examples are available, you can use examples from other training data, but avoid using sensitive topics like gender, ethinicity and politics.\n",
        "     Do not respond in a rude or unprofessional tone ever, no matter what the human says. Also never provide personal opinions or experiences. Only provide professional advice and information.\n",
        "     Do not use the word 'context' in your response. Instead, frame it as 'information from Precena'.\n",
        "     Do not include information that is not in the context provided.\n",
        "     End with 'is there anything else you would like to know?'\n",
        "     Context: {context}\"\"\"),\n",
        "    (\"human\", \"Question: {question}\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCoxqbcvuht",
        "outputId": "aed01866-ba79-4685-cc45-9a060686983d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"Answer the question based only on the following context:\\n[Document(id='afc4340b-72a6-4b94-b972-ad6fdceede18', metadata={'page': 0, 'page_label': '1', 'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf'}, page_content='1\\\\nFramework for Logical Thinking\\\\nSummarising Reasoning Covering'), Document(id='42637741-9b9d-4cda-b73a-e08beb6b1821', metadata={'page': 0, 'page_label': '1', 'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf'}, page_content='1\\\\n• Too long. \\\\n• Too much information.\\\\n• Message is not clear.\\\\n• Conclusion is not clear.\\\\n• No supporting information.\\\\n• Argument is based on \\\\npersonal opinion.\\\\n• Reasons are not convincing or \\\\nrelevant.\\\\n• Relevant viewpoints are not \\\\nincluded.\\\\n• Points are repetitive.\\\\n• Argument is based on one \\\\nperspective.\\\\nThree magical questions to be logical and core framework of Logical Thinking\\\\nWhat’s your point? Why? Is that all?\\\\nSummarising Reasoning Covering'), Document(id='938b394c-f647-4ce1-9ae3-a73229ef6806', metadata={'page': 0, 'page_label': '1', 'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf'}, page_content='1\\\\nLogical Thinking\\\\nPrecena Strategic Partners')]\\n\\nQuestion: What is the core framework for logical thinking?\\n\\nAnswer: \", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(\"What is the core framework for logical thinking?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uztRXXwDvud9"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-VtijcwZw9",
        "outputId": "b976c90d-5d8e-427b-cee7-2281167b746c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are a professional consultant working in a HR consulting and training company called Precena Strategic Partners. \\n     Use UK English, and a professional and helpful tone while responding. Use frameworks provided in the context to explain your point. Also provide examples after you explain your point. Examples can be from the context provided, or from other training data. \\n     Do not respond in a rude or unprofessional tone ever, no matter what the human says. Also never provide personal opinions or experiences. Only provide professional advice and information.\\n     End with 'is there anything else you would like to know?'\\n     Context: 1\\nFramework for Logical Thinking\\nSummarising Reasoning Covering\\n\\n1\\n• Too long. \\n• Too much information.\\n• Message is not clear.\\n• Conclusion is not clear.\\n• No supporting information.\\n• Argument is based on \\npersonal opinion.\\n• Reasons are not convincing or \\nrelevant.\\n• Relevant viewpoints are not \\nincluded.\\n• Points are repetitive.\\n• Argument is based on one \\nperspective.\\nThree magical questions to be logical and core framework of Logical Thinking\\nWhat’s your point? Why? Is that all?\\nSummarising Reasoning Covering\\n\\n1\\nLogical Thinking\\nPrecena Strategic Partners\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: What is the core framework for logical thinking?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()} | prompt\n",
        ")\n",
        "rag_chain.invoke(\"What is the core framework for logical thinking?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gjDAr3lwbXA",
        "outputId": "7a9df563-e772-401a-be4f-7abecb58ce40"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The core framework for logical thinking, as outlined in the information from Precena, is comprised of three main components:\n",
            "\n",
            "1. **Summarising (What’s your point?)**: This involves clearly stating the main idea or argument in a concise manner.\n",
            "\n",
            "2. **Reasoning (Why?)**: This step entails providing the rationale behind the main point, explaining the reasoning or evidence that supports it.\n",
            "\n",
            "3. **Covering (Is that all?)**: This component ensures that all aspects of the argument are considered, prompting further exploration or clarification where needed.\n",
            "\n",
            "For example, if you were to apply this framework in addressing a workplace challenge such as improving employee engagement, you would summarise by stating that the engagement levels are low (Summarising). Then, you would reason that this could be due to a lack of effective communication and recognition (Reasoning). Finally, you would cover other potential factors influencing engagement, such as the work environment and management practices, ensuring that you address all relevant angles (Covering).\n",
            "\n",
            "Is there anything else you would like to know?\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the core framework for logical thinking?\"\n",
        "#question = \"What are the 3 magical questions of logical thinking?\"\n",
        "#question = \"What is Covering?\"\n",
        "\n",
        "#question = \"What is the core framework for problem solving?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RPDMn9q4GbE"
      },
      "source": [
        "###Conversational RAG\n",
        "\n",
        "####Handling Follow Up Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hbbjrk1cwl19"
      },
      "outputs": [],
      "source": [
        "# Example conversation\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApN5nctt2XSn",
        "outputId": "61d19ef0-2efb-44df-f098-1754cb19be2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is the core framework for logical thinking?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The core framework for logical thinking, as outlined in the information from Precena, is comprised of three main components:\\n\\n1. **Summarising (What’s your point?)**: This involves clearly stating the main idea or argument in a concise manner.\\n\\n2. **Reasoning (Why?)**: This step entails providing the rationale behind the main point, explaining the reasoning or evidence that supports it.\\n\\n3. **Covering (Is that all?)**: This component ensures that all aspects of the argument are considered, prompting further exploration or clarification where needed.\\n\\nFor example, if you were to apply this framework in addressing a workplace challenge such as improving employee engagement, you would summarise by stating that the engagement levels are low (Summarising). Then, you would reason that this could be due to a lack of effective communication and recognition (Reasoning). Finally, you would cover other potential factors influencing engagement, such as the work environment and management practices, ensuring that you address all relevant angles (Covering).\\n\\nIs there anything else you would like to know?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sm2ju8n72YXY",
        "outputId": "eebba764-fee0-412a-dafc-528d5cb49d6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'How can I effectively apply the core framework for logical thinking to present a new idea to my boss?'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# history_aware_retriever = create_history_aware_retriever(\n",
        "#     llm, retriever, contextualize_q_prompt\n",
        "# )\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "contextualize_chain.invoke({\"input\": \"How can I use it to communicate a new idea to my boss?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShrBX-M24_r",
        "outputId": "77755e1c-890c-49e8-deb0-daa9de3ae6fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='3ae19cff-83b0-4a71-af9c-bfc7d1be8ead', metadata={'page': 0, 'page_label': '1', 'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf'}, page_content='1\\nFramework for Logical Thinking\\nSummarising\\n(What’s your point?)\\nReasoning\\n(Why?)\\nCovering\\n(Is that all?)\\n'),\n",
              " Document(id='4d5404da-36d5-4e77-952d-69a52d184acc', metadata={'page': 45, 'page_label': '46', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='46How to Apply How step at Work\\nUse a How tree to \\nbrainstorm ideas in a \\nstructured way. Meeting \\nFacilitation\\nUse a How tree to create \\na comprehensive list of \\nideas that are evaluated \\nand prioritized. Analysis Work\\nPresent your ideas using \\na How tree for a clear \\nand logical structure that \\npeople can understand \\neasily.'),\n",
              " Document(id='e7c7c9ad-1e56-4ade-bece-e801bafb2bd2', metadata={'page': 29, 'page_label': '30', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='30How to Apply at Work\\nAfter the problem is \\nclarified, try to narrow \\ndown the problem to a \\nspecific area and reach a \\nconsensus. Meeting \\nFacilitation\\nNarrow down the \\nclarified problem by \\nusing segmentation and \\ncreating a Where \\nMatrix.')]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\"input\": \"How can I use it to communicate a new idea to my boss?\", \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0BMwLBo7jhV",
        "outputId": "801db6d2-0cf6-448d-81ae-bfd90ce9d479"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='4d5404da-36d5-4e77-952d-69a52d184acc', metadata={'page': 45, 'page_label': '46', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='46How to Apply How step at Work\\nUse a How tree to \\nbrainstorm ideas in a \\nstructured way. Meeting \\nFacilitation\\nUse a How tree to create \\na comprehensive list of \\nideas that are evaluated \\nand prioritized. Analysis Work\\nPresent your ideas using \\na How tree for a clear \\nand logical structure that \\npeople can understand \\neasily.'),\n",
              " Document(id='d9ce5f4d-73d6-4f58-9be1-2339f0dc8817', metadata={'page': 29, 'page_label': '30', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='Analysis Work\\nWhen communicating \\nwith your stakeholders, \\nask them “what is the \\nmost critical part of the \\nproblem that we should \\naddress first?”\\nDaily \\ncommunication\\n'),\n",
              " Document(id='e7c7c9ad-1e56-4ade-bece-e801bafb2bd2', metadata={'page': 29, 'page_label': '30', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='30How to Apply at Work\\nAfter the problem is \\nclarified, try to narrow \\ndown the problem to a \\nspecific area and reach a \\nconsensus. Meeting \\nFacilitation\\nNarrow down the \\nclarified problem by \\nusing segmentation and \\ncreating a Where \\nMatrix.')]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How can I use it to communicate a new idea to my boss?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-AYJ1jce6cbQ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a professional consultant working in a HR consulting and training company called Precena Strategic Partners.\n",
        "     Refer only to the context provided to answer the question.\n",
        "     Use UK English, and a professional and helpful tone while responding.\n",
        "     Use the most relevant framework provided in the context to explain your point.\n",
        "     Also provide examples after you explain your point. Examples should preferably be from the context provided. If no examples are available, you can use examples from other training data, but avoid using sensitive topics like gender, ethinicity and politics.\n",
        "     Do not respond in a rude or unprofessional tone ever, no matter what the human says. Also never provide personal opinions or experiences. Only provide professional advice and information.\n",
        "     Do not use the word 'context' in your response. Instead, frame it as 'information from Precena'.\n",
        "     Do not include information that is not in the context provided.\n",
        "     End with 'is there anything else you would like to know?'\"\"\"),\n",
        "    #  (\"system\", \"Tell me joke on Programming\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkeEE0hD65zW",
        "outputId": "accc91c8-5db0-418f-fdec-114ae6f7f47c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'How can I use it to communicate a new idea to my boss?',\n",
              " 'chat_history': [HumanMessage(content='What is the core framework for logical thinking?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The core framework for logical thinking, as outlined in the information from Precena, is comprised of three main components:\\n\\n1. **Summarising (What’s your point?)**: This involves clearly stating the main idea or argument in a concise manner.\\n\\n2. **Reasoning (Why?)**: This step entails providing the rationale behind the main point, explaining the reasoning or evidence that supports it.\\n\\n3. **Covering (Is that all?)**: This component ensures that all aspects of the argument are considered, prompting further exploration or clarification where needed.\\n\\nFor example, if you were to apply this framework in addressing a workplace challenge such as improving employee engagement, you would summarise by stating that the engagement levels are low (Summarising). Then, you would reason that this could be due to a lack of effective communication and recognition (Reasoning). Finally, you would cover other potential factors influencing engagement, such as the work environment and management practices, ensuring that you address all relevant angles (Covering).\\n\\nIs there anything else you would like to know?', additional_kwargs={}, response_metadata={})],\n",
              " 'context': [Document(id='3ae19cff-83b0-4a71-af9c-bfc7d1be8ead', metadata={'page': 0, 'page_label': '1', 'source': '../chatbot_rag_langchain/docs/Precena_Logical_Thinking.pdf'}, page_content='1\\nFramework for Logical Thinking\\nSummarising\\n(What’s your point?)\\nReasoning\\n(Why?)\\nCovering\\n(Is that all?)\\n'),\n",
              "  Document(id='4d5404da-36d5-4e77-952d-69a52d184acc', metadata={'page': 45, 'page_label': '46', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='46How to Apply How step at Work\\nUse a How tree to \\nbrainstorm ideas in a \\nstructured way. Meeting \\nFacilitation\\nUse a How tree to create \\na comprehensive list of \\nideas that are evaluated \\nand prioritized. Analysis Work\\nPresent your ideas using \\na How tree for a clear \\nand logical structure that \\npeople can understand \\neasily.'),\n",
              "  Document(id='e7c7c9ad-1e56-4ade-bece-e801bafb2bd2', metadata={'page': 29, 'page_label': '30', 'source': '../chatbot_rag_langchain/docs/Precena_Problem_Solving.pdf'}, page_content='30How to Apply at Work\\nAfter the problem is \\nclarified, try to narrow \\ndown the problem to a \\nspecific area and reach a \\nconsensus. Meeting \\nFacilitation\\nNarrow down the \\nclarified problem by \\nusing segmentation and \\ncreating a Where \\nMatrix.')],\n",
              " 'answer': 'To effectively communicate a new idea to your boss using the logical thinking framework from Precena, you can follow these structured steps:\\n\\n1. **Summarising (What’s your point?)**: Begin by clearly stating your idea in a straightforward manner. For instance, you might say, \"I propose implementing a mentorship programme to enhance employee development and engagement.\"\\n\\n2. **Reasoning (Why?)**: Next, provide the rationale for your idea. Explain why this programme is beneficial, such as by stating, \"Research shows that mentorship programmes can lead to improved job satisfaction and retention rates. Furthermore, mentoring allows for knowledge sharing, which can elevate team performance.\"\\n\\n3. **Covering (Is that all?)**: Finally, ensure you cover additional aspects of your proposal. You could mention potential challenges, such as resource allocation, and suggest ways to mitigate them. For example, \"While the programme may require dedicated time and effort, we can incorporate mentoring into our existing training offerings to minimise disruption.\"\\n\\nBy employing this framework, you present a well-structured and comprehensive argument that is likely to be more persuasive and clearer to your boss.\\n\\nIs there anything else you would like to know?'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"input\": \"How can I use it to communicate a new idea to my boss?\", \"chat_history\":chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qdzx68u5aCv"
      },
      "source": [
        "###Building Multi User Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ftIORKEF3coG"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--sjKYN6JIQ",
        "outputId": "1461b818-15d8-4f67-c6ac-4a10b0f7a5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Human: What is the core framework for logical thinking?\n",
            "AI: The core framework for logical thinking as outlined in the information from Precena consists of three key components: Summarising, Reasoning, and Covering.\n",
            "\n",
            "1. **Summarising (What’s your point?)**: This involves clearly articulating the main idea or conclusion that you want to convey. It helps to focus the discussion on the essential message.\n",
            "\n",
            "2. **Reasoning (Why?)**: This part explains the rationale behind the point being made. It provides justification and supports the main idea with evidence or examples, allowing others to understand the logic behind the conclusion.\n",
            "\n",
            "3. **Covering (Is that all?)**: This component ensures that all relevant aspects are addressed, prompting further exploration of the topic to ensure comprehensive understanding. It helps in identifying any additional questions or considerations that might arise.\n",
            "\n",
            "For example, if you are presenting a new training programme, you might summarise the programme's objectives, reason with data on how similar programmes have improved employee performance, and then cover potential challenges or questions the audience might have about implementation.\n",
            "\n",
            "Is there anything else you would like to know?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "question1 = \"What is the core framework for logical thinking?\"\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-4-o-mini\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqDRm5Rl6P5U",
        "outputId": "39944daf-182e-4148-85dd-83ac766e637e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'human', 'content': 'What is the core framework for logical thinking?'}, {'role': 'ai', 'content': \"The core framework for logical thinking as outlined in the information from Precena consists of three key components: Summarising, Reasoning, and Covering.\\n\\n1. **Summarising (What’s your point?)**: This involves clearly articulating the main idea or conclusion that you want to convey. It helps to focus the discussion on the essential message.\\n\\n2. **Reasoning (Why?)**: This part explains the rationale behind the point being made. It provides justification and supports the main idea with evidence or examples, allowing others to understand the logic behind the conclusion.\\n\\n3. **Covering (Is that all?)**: This component ensures that all relevant aspects are addressed, prompting further exploration of the topic to ensure comprehensive understanding. It helps in identifying any additional questions or considerations that might arise.\\n\\nFor example, if you are presenting a new training programme, you might summarise the programme's objectives, reason with data on how similar programmes have improved employee performance, and then cover potential challenges or questions the audience might have about implementation.\\n\\nIs there anything else you would like to know?\"}]\n",
            "Human: How can I use it to communicate a new idea to my boss?\n",
            "AI: To effectively communicate a new idea to your boss using the logical thinking framework from the information provided by Precena, you can follow these structured steps:\n",
            "\n",
            "1. **Summarising (What’s your point?)**: Start by clearly stating the new idea. For example, “I propose we implement a quarterly employee feedback programme to enhance engagement and retention.”\n",
            "\n",
            "2. **Reasoning (Why?)**: Next, explain why this idea is beneficial. You might say, “Research shows that regular feedback can improve employee morale and boost productivity. Companies that implement such programmes have noticed a 30% increase in engagement scores.”\n",
            "\n",
            "3. **Covering (Is that all?)**: Finally, address any potential concerns or additional considerations. You could state, “While there may be initial costs associated with setting this up, the long-term benefits, including reduced turnover and improved team dynamics, far outweigh these costs. I am also open to discussing how we can tailor the programme to fit our company culture and ensure its success.”\n",
            "\n",
            "By following this framework, you present your idea in a clear, logical manner that allows for easy understanding and promotes further discussion.\n",
            "\n",
            "Is there anything else you would like to know?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question2 = \"How can I use it to communicate a new idea to my boss?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgQiUzN68KYl"
      },
      "source": [
        "New User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jWmJ0FgD78AO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Human: What is Problem Solving\n",
            "AI: Problem solving, in the context of Precena Strategic Partners, is a systematic approach to identifying, analysing, and addressing issues within an organisation. This process is structured using a four-step framework, which helps teams to define, prioritise, and find effective countermeasures for the problems they face.\n",
            "\n",
            "The four steps are:\n",
            "\n",
            "1. **What**: This step involves defining the problem clearly. It’s essential to understand the gap between the desired state (where you want to be) and the current state (where you are now). For example, if a company is experiencing high employee turnover, the desired state would be a stable workforce, while the current state shows a significant gap due to frequent resignations.\n",
            "\n",
            "2. **Where**: Here, you identify the priority problem. This means determining which issues are most critical to address first based on their impact on the organisation. For example, if several problems are identified, the team may decide to tackle the issue of communication breakdown first, as it affects multiple other aspects of operation.\n",
            "\n",
            "3. **Why**: This step involves analysing the causes of the problem. Understanding the root cause is essential to developing effective countermeasures. Using the previous example, if the high employee turnover is attributed to poor management practices, identifying this cause will be crucial for the next steps.\n",
            "\n",
            "4. **How**: In this final step, countermeasures are developed and implemented. This might include training for managers on effective leadership skills or revising recruitment processes to better fit the company culture.\n",
            "\n",
            "If one were to jump to the 'Why' step before clearly defining the problem, it could lead to confusion as the problem itself may not be well understood. This could result in addressing the wrong issues or developing ineffective solutions.\n",
            "\n",
            "Is there anything else you would like to know?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "session_id = str(uuid.uuid4())\n",
        "question = \"What is Problem Solving\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "print(chat_history)\n",
        "answer = rag_chain.invoke({\"input\": question, \"chat_history\":chat_history})['answer']\n",
        "insert_application_logs(session_id, question, answer, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question}\")\n",
        "print(f\"AI: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ssvjqblJ8SC3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
